from multiprocessing import context
import os
import pandas as pd
import numpy as np
import json
from tqdm.notebook import tqdm 
tqdm.pandas()
import warnings
warnings.filterwarnings("ignore")
import re
from typing import List, Dict, Any

from huggingface_hub import login
from datasets import load_dataset
import faiss

from openai import OpenAI
from datasets import Dataset

# Config
CONFIG_PATH = "../config.json"
with open(CONFIG_PATH, "r") as f:
    config = json.load(f)
UPSTAGE_API = config["UPSTAGE_API"]
TAVILY_API_KEY = config["TAVILY_API_KEY"]
HUGGING_FACE_TOKEN = config["HUGGING_FACE_TOKEN"]
login(token=HUGGING_FACE_TOKEN)

# Config: Model
SOLAR_MODEL = 'solar-pro2'
LLAMA_MODEL = 'meta-llama/Llama-3.2-3B-Instruct'

# Config: Data
# Config: Data
def select_dataset_config(dataset_name: str) -> Dict[str, str]:
    if dataset_name == "FINANCE":
        return {
            "QA_PATH": config["FINANCE_QA_PATH"],
            "BENCHMARK_PATH": config["FINANCE_BENCHMARK_PATH"],
            "BENCHMARK_VALID_PATH": config["FINANCE_BENCHMARK_VALID_PATH"],
            "BENCHMARK_VALID_FILTERING_PATH": config["FINANCE_BENCHMARK_VALID_FILTERING_PATH"],
        }
    elif dataset_name == "PIT":
        return {
            "QA_PATH": config["PIT_QA_PATH"],
            "BENCHMARK_PATH": config["PIT_BENCHMARK_PATH"],
            "BENCHMARK_RAG_PATH": config["PIT_BENCHMARK_RAG_PATH"],
            "BENCHMARK_VALID_PATH": config["PIT_BENCHMARK_VALID_PATH"],
            "BENCHMARK_VALID_FILTERING_PATH": config["PIT_BENCHMARK_VALID_FILTERING_PATH"],
        }
    elif dataset_name == "REPORT":
        return {
            "QA_PATH": config["REPORT_QA_PATH"],
            "BENCHMARK_PATH": config["REPORT_BENCHMARK_PATH"],
            "BENCHMARK_RAG_PATH": config["REPORT_BENCHMARK_RAG_PATH"],
            "BENCHMARK_VALID_PATH": config["REPORT_BENCHMARK_VALID_PATH"],
            "BENCHMARK_VALID_FILTERING_PATH": config["REPORT_BENCHMARK_VALID_FILTERING_PATH"],
        }
    elif dataset_name == "KRX":
        return {
            "QA_PATH": config["KRX_QA_PATH"],
            "BENCHMARK_PATH": config["KRX_BENCHMARK_PATH"],
            "BENCHMARK_RAG_PATH": config["KRX_BENCHMARK_RAG_PATH"],
            "BENCHMARK_VALID_PATH": config["KRX_BENCHMARK_VALID_PATH"],
            "BENCHMARK_VALID_FILTERING_PATH": config["KRX_BENCHMARK_VALID_FILTERING_PATH"],
        }
    else:
        raise ValueError(f"Unknown dataset name: {dataset_name}")

# Config: Prompt
GENERATE_QA_PROMPT_PATH = config["GENERATE_QA_PROMPT_PATH"]
EVAL_DOMAIN_PROMPT_PATH = config["EVAL_DOMAIN_PROMPT_PATH"]
EVAL_QUALITY_PROMPT_PATH = config["EVAL_QUALITY_PROMPT_PATH"]
EVAL_DIFFICULTY_PROMPT_PATH = config["EVAL_DIFFICULTY_PROMPT_PATH"]


### Process Data ###
def load_prompt(path) -> str:
    """
    í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜.

    Parameters:
        path (str): í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ)
    Returns:
        str: í”„ë¡¬í”„íŠ¸ ë‚´ìš©    
    """
    with open(path, "r", encoding="utf-8") as f:
        return f.read()
    
def chunk_text(text: str, chunk_size: int = 5, chunk_overlap: int = 100) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì¼ì • ê¸¸ì´(chunk_size) ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜.
    ê° ì²­í¬ ì‚¬ì´ì— ì¼ì • ë¶€ë¶„(chunk_overlap)ì„ ê²¹ì¹˜ê²Œ í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€ë¥¼ ë•ëŠ”ë‹¤.
    
    Parameters:
        text (str): ë‚˜ëˆŒ ì „ì²´ ì…ë ¥ í…ìŠ¤íŠ¸
        chunk_size (int): ê° ì²­í¬ì˜ ìµœëŒ€ í† í° ë‹¨ìœ„ ê¸¸ì´ (ê¸°ë³¸ê°’=5)
        chunk_overlap (int): ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” í† í° ìˆ˜ (ê¸°ë³¸ê°’=100)
    
    Returns:
        List[str]: ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    tokens = re.findall(r"\S+\s*", text)
    out = []
    i = 0
    while i < len(tokens):
        chunk = "".join(tokens[i:i+chunk_size]).strip()
        if chunk:
            out.append(chunk)
        if i + chunk_size >= len(tokens):
            break
        i += max(1, chunk_size - chunk_overlap)
    return out or [text]


### Generate Benchmark ###
def generate_for_contexts(
    contexts: List[str],
    api_key: str,
    save_path: str,
    model: str = SOLAR_MODEL
) -> List[dict]:
    """
    ê° contextì— ëŒ€í•´ ë‹¨ í•˜ë‚˜ì˜ Q-A ìŒì„ ìƒì„±í•˜ê³ 
    Context, Question, Answer í•„ë“œë§Œ í¬í•¨ëœ JSON ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """

    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")
    sys_prompt = load_prompt(GENERATE_QA_PROMPT_PATH)

    results = []
    for context in contexts:
        prompt = f"[CONTEXT]\n{context}\n\nìœ„ ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° í•œ ê°œì˜ ì§ˆë¬¸ê³¼ ì •ë‹µì„ ìƒì„±í•˜ë¼."

        response_format = {
            "type": "json_schema",
            "json_schema": {
                "name": "qa_pair",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "Context": {"type": "string"},
                        "Question": {"type": "string"},
                        "Answer": {"type": "string"},
                    },
                    "required": ["Context", "Question", "Answer"],
                },
            },
        }

        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt},
        ]

        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.2,
            stream=False,
            response_format=response_format
        )

        result = json.loads(response.choices[0].message.content)
        results.append(result)

    # ë°ì´í„° ì €ì¥
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    return results


### Top-k Similar Docs Retrieval for RAG ###
def get_embeddings(client: OpenAI, texts: List[str], mode: str = "query") -> List[List[float]]:
    """Get embeddings using Solar embedding model.
    
    Args:
        client: OpenAI client configured for Upstage API
        texts: List of input strings
        mode: "query" or "passage"
        
    Returns:
        List of embedding vectors
    """
    if mode == "query":
        model = "embedding-query"
    elif mode == "passage":
        model = "embedding-passage"
    else:
        raise ValueError("mode must be 'query' or 'passage'")
    
    response = client.embeddings.create(
        model=model,
        input=texts
    )
    return [embedding.embedding for embedding in response.data]

def add_similar_docs_to_benchmark(benchmark, all_contexts, client, get_embeddings, k=3) -> List[Dict[str, Any]]:
    """
    Questionì— ëŒ€í•´ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ benchmarkì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜

    Parameters:
        benchmark: ì§ˆë¬¸/ë¬¸ë§¥ dict ë¦¬ìŠ¤íŠ¸
        all_contexts: ì¸ë±ìŠ¤ì— ë“¤ì–´ê°ˆ ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        client, get_embeddings: ì„ë² ë”© ìƒì„±ì— í•„ìš”í•œ ê°ì²´/í•¨ìˆ˜
        k: Top-K ìœ ì‚¬ ë¬¸ì„œ ê°œìˆ˜
    Returns:
        benchmark: ì§ˆë¬¸/ë¬¸ë§¥ dict ë¦¬ìŠ¤íŠ¸
    """
    # 1. ì „ì²´ ë¬¸ì„œ ì„ë² ë”© (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì€ all_contexts ì‚¬ìš©)
    embeddings = get_embeddings(client, all_contexts, mode="passage")
    document_embeddings = np.array(embeddings).astype(np.float32)

    # 2. ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    dimension = document_embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)
    faiss.normalize_L2(document_embeddings)
    index.add(document_embeddings)

    # 3. ê° ì§ˆë¬¸ë³„ë¡œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ë° benchmarkì— ì¶”ê°€
    for item in benchmark:
        question = item['Question']
        question_embedding = get_embeddings(client, [question], mode="query")[0]
        question_embedding = np.array(question_embedding).astype(np.float32)
        question_embedding = question_embedding.reshape(1, -1)
        faiss.normalize_L2(question_embedding)
        similarity_scores, indices = index.search(question_embedding, k)
        retrieved_contexts = [all_contexts[idx] for idx in indices[0]]
        item['Context_RAG'] = retrieved_contexts
    return benchmark


def detect_domain_generate_judge_prompt(
    qa_data: List[str],
    api_key: str,
) -> List[dict]:
    """
    ê° contextì— ëŒ€í•´ ë„ë©”ì¸ì„ ìë™ íŒë³„í•˜ê³ ,
    íŒë³„ëœ ë„ë©”ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ Questionì´ ë„ë©”ì¸ì— ì í•©í•œì§€ í‰ê°€í•˜ëŠ” Promptë¥¼ ìƒì„±
    """

    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")
    sys_prompt = load_prompt(EVAL_DOMAIN_PROMPT_PATH)

    results = []
    for qa in qa_data:
        context = qa['Context_RAG']
        question = qa['Question']
        answer = qa['Answer']
        prompt = f"ë„ë©”ì¸ì„ ìë™ íŒë³„í•˜ê³ , Questionì´ ë„ë©”ì¸ì— ì í•©í•œì§€ í‰ê°€í•˜ëŠ” Promptë¥¼ ì‘ì„±í•˜ë¼.\n\nContext: {context}\nQuestion: {question}\nAnswer: {answer}"

        response_format = {
            "type": "json_schema",
            "json_schema": {
                "name": "domain_prompt",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "Domain": {"type": "string"},
                        "Domain_Prompt": {"type": "string"},
                    },
                    "required": ["Domain", "Domain_Prompt"],
                },
            },
        }

        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt},
        ]

        response = client.chat.completions.create(
            model=SOLAR_MODEL,
            messages=messages,
            temperature=0.2,
            stream=False,
            response_format=response_format
        )

        result = json.loads(response.choices[0].message.content)
        results.append(result)
    return results

def evaluate_domain_validity(
    qa_data: List[str],
    domain_info: List[str],
    api_key: str,
) -> List[dict]:
    """
    ë„ë©”ì¸ í‰ê°€ Promptì— ë”°ë¼ Question/Answer ìŒì´ ë„ë©”ì¸ì— ì í•©í•œì§€ í‰ê°€
    """

    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")

    results = []
    for i in range(len(qa_data)):
        question = qa_data[i]['Question']
        answer = qa_data[i]['Answer']
        domain = domain_info[i]['Domain']
        domain_prompt = domain_info[i]['Domain_Prompt']
        prompt = f"Question/Answer ìŒì´ ë„ë©”ì¸ì— ì í•©í•œì§€ í‰ê°€í•˜ë¼.\n\nQuestion: {question}\nAnswer: {answer}"

        response_format = {
            "type": "json_schema",
            "json_schema": {
                "name": "evaluate_domain_prompt",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "Domain": {"type": "string"},
                        "Domain_Validity": {"type": "integer"},
                        "Domain_Validity_Reason": {"type": "string"},
                    },
                    "required": ["Domain", "Domain_Validity", "Domain_Validity_Reason"],
                },
            },
        }

        messages = [
            {"role": "system", "content": domain_prompt},
            {"role": "user", "content": prompt},
        ]

        response = client.chat.completions.create(
            model=SOLAR_MODEL,
            messages=messages,
            temperature=0.2,
            stream=False,
            response_format=response_format
        )

        result = json.loads(response.choices[0].message.content)
        results.append(result)
        
    return results

def evaluate_quality(
    qa_data: List[str],
    api_key: str,
) -> List[dict]:
    """
    QA ë°ì´í„°ì˜ í’ˆì§ˆì„ í‰ê°€
    **í‰ê°€ ê¸°ì¤€**
    1. Contextì™€ Questionì˜ ê´€ë ¨ì„±
    2. Questionì˜ ëª…í™•ì„±ê³¼ êµ¬ì²´ì„±
    3. Answerì˜ ì •í™•ì„±ê³¼ ì™„ì„±ë„
    4. ì „ì²´ì ì¸ ë…¼ë¦¬ì  ì¼ê´€ì„±
    5. ì •ë³´ì˜ ìœ ìš©ì„±ê³¼ ê°€ì¹˜
    """

    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")
    sys_prompt = load_prompt(EVAL_QUALITY_PROMPT_PATH)

    results = []
    for i in range(len(qa_data)):
        context = qa_data[i]['Context_RAG']
        question = qa_data[i]['Question']
        answer = qa_data[i]['Answer']
        prompt = f"Question/Answer ìŒì˜ í’ˆì§ˆì„ í‰ê°€í•˜ë¼.\n\nContext: {context}\nQuestion: {question}\nAnswer: {answer}"

        response_format = {
            "type": "json_schema",
            "json_schema": {
                "name": "evaluate_quality",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "Quality_Score": {"type": "integer"},
                        "Quality_Reason": {"type": "string"},
                    },
                    "required": ["Quality_Score", "Quality_Reason"],
                },
            },
        }

        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt},
        ]

        response = client.chat.completions.create(
            model=SOLAR_MODEL,
            messages=messages,
            temperature=0.2,
            stream=False,
            response_format=response_format
        )

        result = json.loads(response.choices[0].message.content)
        results.append(result)
        
    return results

def evaluate_difficulty(
    qa_data: List[str],
    api_key: str,
) -> List[dict]:
    """
    QA ë°ì´í„°ì˜ ë‚œì´ë„ë¥¼ í‰ê°€
    **í‰ê°€ ê¸°ì¤€**:
    1ì : very easy 
    2ì : easy 
    3ì : medium 
    4ì : hard 
    5ì : very hard 
    """

    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")
    sys_prompt = load_prompt(EVAL_DIFFICULTY_PROMPT_PATH)

    results = []
    for i in range(len(qa_data)):
        question = qa_data[i]['Question']
        answer = qa_data[i]['Answer']
        prompt = f"Question/Answer ìŒì˜ ë‚œì´ë„ë¥¼ í‰ê°€í•˜ë¼.\n\nQuestion: {question}\nAnswer: {answer}"

        response_format = {
            "type": "json_schema",
            "json_schema": {
                "name": "evaluate_difficulty",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "Difficulty": {"type": "integer"},
                        "Difficulty_Reason": {"type": "string"},
                    },
                    "required": ["Difficulty", "Difficulty_Reason"],
                },
            },
        }

        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt},
        ]

        response = client.chat.completions.create(
            model=SOLAR_MODEL,
            messages=messages,
            temperature=0.2,
            stream=False,
            response_format=response_format
        )

        result = json.loads(response.choices[0].message.content)
        results.append(result)
        
    return results

def qa_evaluate(qa_data: List[str], api_key: str, save_path: str) -> Dict[str, Any]:
    """
    QA ë°ì´í„°ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜: ë„ë©”ì¸ ì í•©ì„±, í’ˆì§ˆ, ë‚œì´ë„ 
    """
    domain_info = detect_domain_generate_judge_prompt(qa_data, api_key=api_key)
    print('finish domain_info')
    domain_results = evaluate_domain_validity(qa_data, domain_info, api_key=api_key)
    print('finish domain_results')
    quality_results = evaluate_quality(qa_data, api_key=api_key)
    print('finish quality_results')
    difficulty_results = evaluate_difficulty(qa_data, api_key=api_key)
    print('finish difficulty_results')

    combined_results = []
    for i in range(len(qa_data)):
        combined_results.append({
            "Context": qa_data[i]['Context'],
            "Context_RAG": qa_data[i]['Context_RAG'],
            "Question": qa_data[i]['Question'],
            "Answer": qa_data[i]['Answer'],
            "Domain": domain_results[i]['Domain'],
            "Domain_Validity": domain_results[i]['Domain_Validity'],
            "Domain_Validity_Reason": domain_results[i]['Domain_Validity_Reason'],
            "Quality_Score": quality_results[i]['Quality_Score'],
            "Quality_Reason": quality_results[i]['Quality_Reason'],
            "Difficulty": difficulty_results[i]['Difficulty'],
            "Difficulty_Reason": difficulty_results[i]['Difficulty_Reason']
        })

    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(combined_results, f, ensure_ascii=False, indent=4)
    return combined_results

### Benchmark Filtering ###
def filter_benchmark_data(
    data: List[Dict],
    domain_threshold: int = 2,
    quality_threshold: int = 2,
    difficulty_threshold: int = 2,
    save_path: str = None
) -> List[Dict]:
    """
    ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
    ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³ , ê¸°ì¤€ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” í•­ëª©ë“¤ë§Œ ì œê±°
    
    í•„í„°ë§ ì¡°ê±´:
    - ë„ë©”ì¸ íƒ€ë‹¹ì„± > thresholdì 
    - í€„ë¦¬í‹° > thresholdì   
    - ë‚œì´ë„ > thresholdì 
    
    Parameters:
        data (List[Dict]): í‰ê°€ëœ QA ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        domain_threshold (int): ë„ë©”ì¸ íƒ€ë‹¹ì„± ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3)
        quality_threshold (int): í’ˆì§ˆ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3)
        difficulty_threshold (int): ë‚œì´ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3)
        save_path (str): í•„í„°ë§ëœ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    
    Returns:
        List[Dict]: í•„í„°ë§ëœ QA ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
    """
    
    original_count = len(data)
    filtered_data = []
    
    # í†µê³„ ì •ë³´
    filter_stats = {
        "domain_passed": 0,
        "quality_passed": 0, 
        "difficulty_passed": 0,
        "all_criteria_passed": 0
    }
    
    for item in data:
        # ê° ê¸°ì¤€ ì ìˆ˜ ì¶”ì¶œ
        domain_score = item.get('Domain_Validity', 0)
        quality_score = item.get('Quality_Score', 0)
        difficulty_score = item.get('Difficulty', 0)
        
        # ê°œë³„ ê¸°ì¤€ í†µê³¼ ì—¬ë¶€
        domain_pass = domain_score > domain_threshold
        quality_pass = quality_score > quality_threshold
        difficulty_pass = difficulty_score > difficulty_threshold
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if domain_pass:
            filter_stats["domain_passed"] += 1
        if quality_pass:
            filter_stats["quality_passed"] += 1
        if difficulty_pass:
            filter_stats["difficulty_passed"] += 1
        
        # ëª¨ë“  ê¸°ì¤€ì„ í†µê³¼í•œ ê²½ìš°ë§Œ í•„í„°ë§ëœ ë°ì´í„°ì— ì¶”ê°€
        if domain_pass and quality_pass and difficulty_pass:
            filter_stats["all_criteria_passed"] += 1
            filtered_data.append(item)  # ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ê·¸ëŒ€ë¡œ ìœ ì§€
    
    # ê²°ê³¼ ì €ì¥
    if save_path:
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(filtered_data, f, ensure_ascii=False, indent=2)
    
    # í•„í„°ë§ ê²°ê³¼ ì¶œë ¥
    filtered_count = len(filtered_data)
    retention_rate = (filtered_count / original_count * 100) if original_count > 0 else 0
    
    print("=" * 50)
    print("ğŸ” ë²¤ì¹˜ë§ˆí¬ í•„í„°ë§ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)
    print(f"ğŸ“Š ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {original_count}ê°œ")
    print(f"âœ… í•„í„°ë§ í›„ ìƒ˜í”Œ ìˆ˜: {filtered_count}ê°œ")
    print(f"ğŸ“ˆ ë³´ì¡´ìœ¨: {retention_rate:.1f}%")
    
    print(f"\nğŸ¯ í•„í„°ë§ ê¸°ì¤€")
    print(f"   â€¢ ë„ë©”ì¸ íƒ€ë‹¹ì„± ì„ê³„ê°’: {domain_threshold}ì  ì´ˆê³¼")
    print(f"   â€¢ í’ˆì§ˆ ì„ê³„ê°’: {quality_threshold}ì  ì´ˆê³¼")
    print(f"   â€¢ ë‚œì´ë„ ì„ê³„ê°’: {difficulty_threshold}ì  ì´ˆê³¼")
    
    print(f"\nğŸ“‹ ê°œë³„ ê¸°ì¤€ í†µê³¼ìœ¨")
    print(f"   â€¢ ë„ë©”ì¸ íƒ€ë‹¹ì„±: {filter_stats['domain_passed']}/{original_count} ({filter_stats['domain_passed']/original_count*100:.1f}%)")
    print(f"   â€¢ í’ˆì§ˆ ê¸°ì¤€: {filter_stats['quality_passed']}/{original_count} ({filter_stats['quality_passed']/original_count*100:.1f}%)")
    print(f"   â€¢ ë‚œì´ë„ ê¸°ì¤€: {filter_stats['difficulty_passed']}/{original_count} ({filter_stats['difficulty_passed']/original_count*100:.1f}%)")
    print(f"   â€¢ ëª¨ë“  ê¸°ì¤€: {filter_stats['all_criteria_passed']}/{original_count} ({filter_stats['all_criteria_passed']/original_count*100:.1f}%)")
    print("=" * 50)
    
    return filtered_data

### Main Process ###

def main():
    choices = ["FINANCE", "PIT", "REPORT", "KRX"]
    print("ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    for idx, choice in enumerate(choices, 1):
        print(f"{idx}. {choice}")
    selection = int(input("ë²ˆí˜¸ ì„ íƒ: "))
    DATA_NAME = choices[selection - 1]
    dataset_config = select_dataset_config(DATA_NAME)

#ë°ì´í„° ë¡œë“œ
    if DATA_NAME == "PIT":
        dfs = load_dataset("ohsuz/PII_text_dataset")
        df = pd.DataFrame(dfs['train'])
        contexts = [i for i in df['text']][:30]
    else:
        contexts = pd.read_csv(dataset_config["QA_PATH"])
        contexts = [i for i in contexts['Context']]

    # ëª¨ë¸ ë¡œë“œ
    client = OpenAI(
        api_key=UPSTAGE_API,
        base_url="https://api.upstage.ai/v1"
    )

    # QA ë²¤ì¹˜ë§ˆí¬ ìƒì„±
    save_path = dataset_config["BENCHMARK_PATH"]
    benchmark = generate_for_contexts(contexts, api_key=UPSTAGE_API, save_path=save_path)

    # ì „ì²´ ë¬¸ì„œ
    all_contexts = []
    for item in benchmark:
        all_contexts.append(item['Context'])

    # RAG ë²¤ì¹˜ë§ˆí¬ ìƒì„±
    rag_benchmark = add_similar_docs_to_benchmark(
        benchmark, all_contexts, client, get_embeddings, k=3
    )
    # RAG ë²¤ì¹˜ë§ˆí¬ ì €ì¥
    with open(dataset_config["BENCHMARK_RAG_PATH"], "w", encoding="utf-8") as f:
        json.dump(rag_benchmark, f, ensure_ascii=False, indent=4)

    # ë°ì´í„° íƒ€ë‹¹ì„± í™•ë³´
    save_path = dataset_config["BENCHMARK_VALID_PATH"]
    benchmark_valid = qa_evaluate(rag_benchmark, api_key=UPSTAGE_API, save_path=save_path)

    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° í•„í„°ë§ ì‹¤í–‰
    filter_save_path = dataset_config["BENCHMARK_VALID_FILTERING_PATH"]
    benchmark_valid_filter = filter_benchmark_data(
        data=benchmark_valid,
        domain_threshold=2,
        quality_threshold=2, 
        difficulty_threshold=2,
        save_path=filter_save_path
    )

    print(f"\nğŸ’¾ í•„í„°ë§ëœ ë°ì´í„°: {len(benchmark_valid_filter)}ê°œ ìƒ˜í”Œ")


    # ë°ì´í„° í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°
    with open(dataset_config["BENCHMARK_VALID_PATH"], "r", encoding="utf-8") as f:
        benchmark_valid = json.load(f)
    benchmark_valid_df = pd.DataFrame(benchmark_valid)
    dataset = Dataset.from_pandas(benchmark_valid_df)
    dataset = dataset.add_column("FILE_NAME", [DATA_NAME] * len(dataset))
    benchmark_id = f"GAYOEN/DOC_RAG_{DATA_NAME}_BENCHMARK"
    dataset.push_to_hub(benchmark_id, token=HUGGING_FACE_TOKEN)
    
    # print('âœ… Upload to Huggingface Hub Done! - ', benchmark_id)

    # ë²¤ì¹˜ë§ˆí¬ í•„í„°ë§ ë°ì´í„°
    with open(dataset_config["BENCHMARK_VALID_FILTERING_PATH"], "r", encoding="utf-8") as f:
        benchmark_valid_filter = json.load(f)
    benchmark_valid_filter_df = pd.DataFrame(benchmark_valid_filter)
    dataset = Dataset.from_pandas(benchmark_valid_filter_df)
    dataset = dataset.add_column("FILE_NAME", [DATA_NAME] * len(dataset))
    benchmark_filter_id = f"GAYOEN/DOC_RAG_{DATA_NAME}_BENCHMARK_FILTERED"
    dataset.push_to_hub(benchmark_filter_id, token=HUGGING_FACE_TOKEN)
    # print('âœ… Upload to Huggingface Hub Done! - Filtered', benchmark_filter_id)

    # ë§í¬ ë°˜í™˜
    benchmark_link = f"https://huggingface.co/datasets/{benchmark_id}"
    benchmark_filter_link = f"https://huggingface.co/datasets/{benchmark_filter_id}"
    print(f"ğŸ”— ë²¤ì¹˜ë§ˆí¬ ë§í¬: {benchmark_link}")
    print(f"ğŸ”— í•„í„°ë§ëœ ë²¤ì¹˜ë§ˆí¬ ë§í¬: {benchmark_filter_link}")
    return {
        "benchmark_link": benchmark_link,
        "benchmark_filter_link": benchmark_filter_link
    }
    
if __name__ == "__main__":
    main()